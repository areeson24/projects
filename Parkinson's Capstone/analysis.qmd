```{r, message=FALSE, warning=FALSE}
# load packages
library(tidyverse)
library(glmnet)
library(caret)
library(MLmetrics)
library(randomForest)
library(gbm)
```

```{r}
## NOTE: assumes you have already saved an RDS object from `wrangling.R` file
# read in clean data (already formatted properly)
pads_data <- readRDS("~/Desktop/2024-2025/School/bst263/finalproj/pads_clean.rds")
# examine variables
summary(pads_data)
```

# Multinomial Logistic Regression (+ L1 Penalty)

Note there are many irrelevant predictors with relatively few subjects, so apply a lasso penalty to serve as a model selector that also helps alleviate over-fitting and reduce variance.

```{r, warning=FALSE}
# outcome vector
y <- pads_data$label
# design matrix
X <- model.matrix(label ~ ., data = pads_data[,-1])[,-1]
# set up cross-validation
ctrl <- trainControl(
  method = "cv", 
  number = 5, 
  classProbs = TRUE, 
  summaryFunction = multiClassSummary, 
  savePredictions = "final"
)
# train multinomial logistic regression model with lasso penalty
set.seed(123)
fit_logit <- train(
  x = X,
  y = y,
  method = "glmnet",
  family = "multinomial",
  trControl = ctrl,
  tuneGrid = expand.grid(alpha = 1, lambda = seq(0.0001, 0.1, length = 100)),
  metric = "Mean_Balanced_Accuracy"
)
# optimal lambda
plot(fit_logit)
best_lambda <- fit_logit$bestTune$lambda
# best CV balanced accuracy
acc_logit <- max(fit_logit$results$Mean_Balanced_Accuracy)
# training data predictions
pred_logit <- predict(fit_logit, s = best_lambda)
# training data confusion matrix
cm_logit <- confusionMatrix(y, pred_logit)
# assess variable importance via standardized coefficients - which were selected?
imp_logit <- coef(fit_logit$finalModel, s = best_lambda)
```

# Bagging

```{r, warning=FALSE}
# fit bagged tree classification model
set.seed(123)
fit_bag <- train(
  x = X,
  y = y,
  method = "treebag",
  trControl = ctrl,
  metric = "Mean_Balanced_Accuracy"
)
# best CV balanced accuracy
acc_bag <- max(fit_bag$results$Mean_Balanced_Accuracy)
# training data predictions
pred_bag <- predict(fit_bag)
# training data confusion matrix
cm_bag <- confusionMatrix(y, pred_bag)
# assess variable importance
imp_bag <- varImp(fit_bag)
plot(imp_bag, top = 10)
```

# Random Forest

```{r, warning=FALSE}
# fit random forest classification model
set.seed(123)
fit_rf <- train(
  x = X,
  y = y,
  method = "rf",
  trControl = ctrl,
  metric = "Mean_Balanced_Accuracy",
  tuneLength = 5
)
# best CV balanced accuracy
acc_rf <- max(fit_rf$results$Mean_Balanced_Accuracy)
# training data predictions
pred_rf <- predict(fit_rf)
# training data confusion matrix
cm_rf <- confusionMatrix(y, pred_rf)
# assess variable importance
imp_rf <- varImp(fit_rf)
plot(imp_rf, top = 10)
```

# Boosting

```{r, warning=FALSE}
# fit boosted tree classification model
set.seed(123)
fit_boost <- train(
  x = X,
  y = y,
  method = "gbm",
  trControl = ctrl,
  verbose = FALSE,          
  metric = "Mean_Balanced_Accuracy",
  tuneGrid = expand.grid(
    n.trees = seq(50, 500, by = 50),
    interaction.depth = c(1, 3, 5),
    shrinkage = c(0.01, 0.1), 
    n.minobsinnode = 10 
  )
)
# best model
fit_boost$bestTune
# best CV balanced accuracy
acc_boost <- max(fit_boost$results$Mean_Balanced_Accuracy)
# training data predictions
pred_boost <- predict(fit_boost)
# training data confusion matrix
cm_boost <- confusionMatrix(y, pred_boost)
# assess variable importance
imp_boost <- varImp(fit_boost)
plot(imp_boost, top = 10)
```
